{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":1854274,"sourceType":"datasetVersion","datasetId":1099611},{"sourceId":8337783,"sourceType":"datasetVersion","datasetId":4951736}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import librosa\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the main path for the dataset\nMain_WAV_Path = Path(\"../input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract MFCC features function\ndef extract_mfcc_features(audio_path_or_waveform, sr=None, max_pad_len=500):\n    if isinstance(audio_path_or_waveform, str):  # If input is a file path\n        y, sr = librosa.load(audio_path_or_waveform, sr=sr)\n    else:  # If input is a raw waveform\n        y = audio_path_or_waveform\n    \n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)  # Rich features\n    \n    # Padding to ensure consistent input size\n    pad_width = max_pad_len - mfccs.shape[1]\n    if pad_width > 0:\n        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n    \n    return mfccs.T","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess data\ndef load_data(dataset_path, augment=False):\n    features = []\n    labels = []\n    \n    for label in os.listdir(dataset_path):\n        label_path = os.path.join(dataset_path, label)\n        if not os.path.isdir(label_path):\n            continue\n        \n        for file in os.listdir(label_path):\n            if file.endswith(\".wav\"):  # Assuming audio files are in WAV format\n                file_path = os.path.join(label_path, file)\n                y, sr = librosa.load(file_path, sr=None)\n                mfcc_features = extract_mfcc_features(y, sr=sr)\n                features.append(mfcc_features)\n                labels.append(label)\n                \n                if augment:\n                    # Data Augmentation\n                    features.append(extract_mfcc_features(add_noise(y), sr=sr))\n                    features.append(extract_mfcc_features(time_shift(y), sr=sr))\n                    labels.extend([label] * 2)  # Same label for augmented samples\n\n    # Convert labels to numerical values\n    label_encoder = LabelEncoder()\n    labels = label_encoder.fit_transform(labels)\n    \n    # Pad or truncate features to have consistent shape\n    max_length = max([x.shape[0] for x in features])\n    features = [np.pad(x, ((0, max_length - x.shape[0]), (0, 0)), mode='constant') for x in features]\n    \n    return np.array(features), np.array(labels), label_encoder\n\n# Augmentation methods\ndef add_noise(y, noise_factor=0.005):\n    noise = np.random.randn(len(y))\n    y_noise = y + noise_factor * noise\n    y_noise = np.clip(y_noise, -1.0, 1.0)\n    return y_noise\n\ndef time_shift(y, shift_max=2):\n    shift = random.randint(-shift_max, shift_max)\n    return np.roll(y, shift)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the data\nemotions = os.listdir(Main_WAV_Path)\nX, y, label_encoder = load_data(Main_WAV_Path, augment=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Flatten features for SVM\nX_train_flattened = X_train.reshape(X_train.shape[0], -1)\nX_test_flattened = X_test.reshape(X_test.shape[0], -1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the SVM model pipeline\nsvm_model = make_pipeline(\n    StandardScaler(),\n    SVC(kernel='rbf', random_state=42, probability=True)  # Using RBF kernel for non-linearity\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the SVM model\nsvm_model.fit(X_train_flattened, y_train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on test set\ny_pred = svm_model.predict(X_test_flattened)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict emotions for custom audio files\ndef predict_emotion_from_file(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    mfcc_features = extract_mfcc_features(audio, sr)\n    mfcc_features_flattened = mfcc_features.flatten().reshape(1, -1)\n    prediction = svm_model.predict(mfcc_features_flattened)\n    predicted_emotion = label_encoder.inverse_transform(prediction)\n    print(f\"Predicted Emotion for {file_path}: {predicted_emotion[0]}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example files\nfiles_to_predict = [\n    \"/kaggle/input/cremad/AudioWAV/1001_DFA_DIS_XX.wav\",\n    \"/kaggle/input/cremad/AudioWAV/1001_DFA_HAP_XX.wav\",\n    \"/kaggle/input/cremad/AudioWAV/1001_IEO_DIS_HI.wav\",\n\n]\n\nfor file in files_to_predict:\n    predict_emotion_from_file(file)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}